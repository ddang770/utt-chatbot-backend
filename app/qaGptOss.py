from langchain_openai import ChatOpenAI
#from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from os import getenv
from dotenv import load_dotenv
from app.vectorstore import VectorStore
from app.config_manager import load_config
import time
import random
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferWindowMemory
from app.memory import PostgresChatMessageHistory

load_dotenv()

# Load chatbot config
config = load_config()

# ======== Logging setup ========
import logging

if config.get("enableLogging", False):
    logging.basicConfig(
        level=getattr(logging, config.get("logLevel", "INFO").upper(), logging.INFO),
        format="%(asctime)s [%(levelname)s] %(message)s",
    )
logger = logging.getLogger(__name__)

# Load LLM theo config
def load_llm():
    llm = ChatOpenAI(
        api_key=getenv("OPENROUTER_API_KEY"),
        base_url=getenv("OPENROUTER_BASE_URL"),
        model=config["model"],
        temperature=config["temperature"]
    )
    print(f"Loaded LLM: {config['model']}")
    return llm

# Tao prompt template
def creat_prompt(template):
    prompt = PromptTemplate(template = template, input_variables=["context", "question"])
    print("Created prompt")
    return prompt

# Read tu VectorDB
def read_vectors_db():
    # Láº¥y instance Ä‘Ã£ load sáºµn hoáº·c táº¡o má»›i
    db = VectorStore.get_instance()
    print("Read vectors from db done")
    return db

# ======== QA Chain ========
def create_qa_chain(prompt, memory, llm, db):
    candidates = [
        "prompt",
        "qa_prompt",
        "qa_template",
        "question_generator_prompt",
        "question_generator_template",
    ]

    for key in candidates:
        try:
            ct_kwargs = {key: prompt}
            chain = ConversationalRetrievalChain.from_llm(
                llm=llm,
                chain_type="stuff",
                retriever=db.as_retriever(
                    search_type="similarity",
                    search_kwargs={"k": config.get("retrieverKSize", 4)}
                ),
                memory=memory,
                return_source_documents=False,
                chain_type_kwargs=ct_kwargs
            )
            logger.info(f"Created QA chain using chain_type_kwargs key: {key}")
            return chain
        except Exception as _:
            # try next key
            continue

    # final fallback: don't pass chain_type_kwargs (let langchain defaults)
    chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        chain_type="stuff",
        retriever=db.as_retriever(
            search_type="similarity",
            search_kwargs={"k": config.get("numDocuments", 4)}
        ),
        memory=memory,
        return_source_documents=False,
    )
    logger.info("Created QA chain (fallback without chain_type_kwargs)")
    return chain

    # llm_chain = ConversationalRetrievalChain.from_llm(
    #     llm=llm,
    #     chain_type= "stuff",
    #     retriever=db.as_retriever(
    #         search_type="similarity",
    #         search_kwargs={"k": config.get("numDocuments", 4)}
    #     ),
    #     memory=memory,
    #     return_source_documents=False,
    #     chain_type_kwargs= {'prompt': prompt}
    # )

    # llm_chain = RetrievalQA.from_chain_type(
    #     llm = llm,
    #     chain_type= "stuff",
    #     retriever=db.as_retriever(
    #         search_type="similarity",
    #         search_kwargs={"k": config.get("numDocuments", 4)}
    #     ),
    #     return_source_documents = False,
    #     chain_type_kwargs= {'prompt': prompt}

    # )
    # print("Created QA chain")
    # return llm_chain

# ======== Prompt ná»™i dung chÃ­nh ========
template = f"""
Báº¡n lÃ  trá»£ lÃ½ tÃ i liá»‡u cá»§a ngÆ°á»i dÃ¹ng. 
Sá»­ dá»¥ng NGá»® Cáº¢NH dÆ°á»›i Ä‘Ã¢y Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i. 
Náº¿u khÃ´ng tÃ¬m tháº¥y thÃ´ng tin phÃ¹ há»£p, hÃ£y tráº£ lá»i: "{config['fallbackMessage']}"

{{context}}

CÃ‚U Há»I: {{question}}
"""

def get_user_memory(user_id: str):
    """Create ConversationBufferWindowMemory and preload last N messages from Postgres."""
    # load messages from DB and populate memory (if any)
    history = PostgresChatMessageHistory(user_id)
    # window size from config
    limit = int(config.get("contextMemoryLimit", 3) or 3)
    # create window memory
    mem = ConversationBufferWindowMemory(
        memory_key="chat_history",
        k=limit,
        return_messages=True,
        #chat_memory=history  # if your PostgresChatMessageHistory matches the required interface
    )

    try:
        msgs = history.get_messages(limit=limit)
        for m in msgs:
            if m["role"] == "user":
                # add_user_message exists on underlying chat memory
                mem.chat_memory.add_user_message(m["content"])
            else:
                mem.chat_memory.add_ai_message(m["content"])
    finally:
        history.close()

    return mem

# ======== Xá»­ lÃ½ truy váº¥n ngÆ°á»i dÃ¹ng ========
def process_query (user_query: str, user_id: str = "anonymous"):
    try: 
        prompt = creat_prompt(template)
        llm = load_llm()
        db = read_vectors_db()
        # build conversational RAG chain with window memory
        memory = None
        if config.get("enableContextMemory", False):
            memory = get_user_memory(user_id)
        llm_chain  = create_qa_chain(prompt, memory, llm, db)

        # Delay pháº£n há»“i (giáº£ láº­p typing indicator)
        delay = config.get("responseDelay", 0)
        if delay > 0:
            time.sleep(delay / 1000)  # convert ms â†’ s


        # Truy váº¥n LLM
        #response = llm_chain.invoke({"query": user_query})
        response = llm_chain.invoke({"question": user_query})
        print(response)
        # response sáº½ tráº£ vá» 1 dict gá»“m 2 key lÃ : query vÃ  result
        if 'answer' not in response:
            return {
                "EM": "Something wrong with query proccess ...",
                "EC": 1,
                "DT": ""
            }

        # Cleanings
        # xá»­ lÃ½ strip vÃ¬ openrouter tráº£ vá» cáº£ reasoning trong result (lá» vcl)
        result = response["answer"].strip()
        # loáº¡i bá» náº¿u cÃ³ prefix "analysis" hay "assistantfinal"
        if "assistantfinal" in result:
            result = result.split("assistantfinal")[-1].strip()
        if "analysis" in result:
            result = result.split("analysis")[-1].strip()

        # Emoji cáº£m xÃºc náº¿u báº­t (sau khi Ä‘Ã£ cÃ³ result)
        if config.get("enableEmojis", False):
            emojis = ["ğŸ¤–", "âœ¨", "ğŸ“˜", "ğŸ’¡", "âœ…", "ğŸ§ "]
            result += " " + random.choice(emojis)

        history = PostgresChatMessageHistory(user_id)
        try:
            # save user message first
            history.add_user_message(user_query)
            # save assistant answer
            history.add_ai_message(result)
        finally:
            history.close()

        # Adding logs
        logger.info(f"[{user_id}] Q: {user_query}")
        logger.info(f"[{user_id}] A: {result}")
            
        return result

    except Exception as e:
        #print(f"Process query error: {str(e)}")  # In ra lá»—i Ä‘á»ƒ debug
        logger.error(f"Process query error: {str(e)}")
        #raise e
        return config["fallbackMessage"]